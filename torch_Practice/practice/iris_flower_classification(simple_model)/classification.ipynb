{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import copy\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision import transforms \n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터 수집"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data = pd.read_csv('/home/hts/A_project/hts_pytorch/data/iris_flower_data/164AB90F4B127F491C.csv')\n",
    "hot_encode_num = {name:i for i, name in enumerate(iris_data['Species'].unique())}\n",
    "iris_data_y = [hot_encode_num[i] for i in iris_data['Species']]\n",
    "\n",
    "iris_data_x = iris_data.drop('Species', axis=1)\n",
    "iris_data_x = (iris_data_x - iris_data_x.mean())/iris_data_x.std()\n",
    "\n",
    "iris_data[['caseno','SepalLength','SepalWidth','PetalLength','PetalWidth']] = iris_data_x\n",
    "iris_data['Species'] = iris_data_y\n",
    "\n",
    "train_iris_data = iris_data.sample(frac=0.8, random_state=42)\n",
    "test_iris_data = iris_data.drop(train_iris_data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 데이터셋 제작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class flower_dataset():\n",
    "    def __init__(self, phase, train_iris_data, test_iris_data):\n",
    "        if phase == 'train':\n",
    "            self.phase_data = train_iris_data\n",
    "        elif phase == 'test':\n",
    "            self.phase_data = test_iris_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.phase_data)\n",
    "    \n",
    "    def x_y_data_collect(self, data):\n",
    "        x_data = data.drop('Species').values\n",
    "        y_data = data['Species'].astype(int)\n",
    "\n",
    "        return x_data, y_data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        x_data,y_data = self.x_y_data_collect(self.phase_data.iloc[index])\n",
    "        x_data = torch.FloatTensor(x_data)\n",
    "        \n",
    "        y_data = torch.tensor(y_data, dtype=torch.long)\n",
    "\n",
    "        return x_data, y_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    x_data_list = []\n",
    "    y_data_list = []\n",
    "\n",
    "    for a, b in batch:\n",
    "        x_data_list.append(a)\n",
    "        y_data_list.append(b)\n",
    "    \n",
    "    x_data_list = torch.stack(x_data_list)\n",
    "    y_data_list = torch.stack(y_data_list)\n",
    "\n",
    "    return x_data_list, y_data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataloader(train_batch_size, val_batch_size):\n",
    "    dataloaders = {}\n",
    "\n",
    "    train_data_set = flower_dataset(phase='train', train_iris_data=train_iris_data, test_iris_data=test_iris_data)\n",
    "    test_data_set = flower_dataset(phase='test', train_iris_data=train_iris_data, test_iris_data=test_iris_data)\n",
    "    \n",
    "    dataloaders['train'] = DataLoader(train_data_set, batch_size= train_batch_size, shuffle=True,collate_fn=collate_fn,num_workers=8, pin_memory=True, drop_last=True)\n",
    "    dataloaders['val'] = DataLoader(test_data_set, batch_size=val_batch_size, shuffle=True, collate_fn=collate_fn,num_workers=8, pin_memory=True, drop_last=True)\n",
    "\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders = build_dataloader(train_batch_size=10,val_batch_size =5)\n",
    "# temp = iter(dataloaders['train'])\n",
    "# temp2 = next(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataloaders = build_dataloader(train_batch_size=3,val_batch_size =5)\n",
    "# for index, batch in enumerate(dataloaders['train']):\n",
    "#     print(index)\n",
    "#     print(\"AAAAAAAAAAAAAAA\")\n",
    "#     print(batch[1])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.input_layer = nn.Linear(5,16)\n",
    "        self.hidden_layer1 = nn.Linear(16, 32)\n",
    "        self.output_layer = nn.Linear(32, 3)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.input_layer(x))\n",
    "        out = self.relu(self.hidden_layer1(out))\n",
    "        out = self.output_layer(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(dataloaders, model, optimizer, device):\n",
    "    total_loss = {'train': 0, 'val':0}\n",
    "    loss = 0.0\n",
    "\n",
    "    for phase in ['train', 'val']:\n",
    "        running_loss = 0.0\n",
    "\n",
    "        if phase =='train':\n",
    "            model.train()\n",
    "        elif phase =='val':\n",
    "            model.eval()\n",
    "        \n",
    "        for index, batch in enumerate(dataloaders[phase]):\n",
    "            x_data = batch[0].to(device)\n",
    "            y_data = batch[1].to(device)\n",
    "\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                predict = model(x_data)\n",
    "\n",
    "                loss = F.cross_entropy(predict, y_data, reduction='mean')\n",
    "\n",
    "                if phase =='train':\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "            running_loss += loss.item()\n",
    "        total_loss[phase] = running_loss/len(dataloaders[phase])\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 시작전 파라미터및 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/300--train_loss : 1.0725281378802132, val_loss : 1.0427854120731355\n",
      "2/300--train_loss : 1.0640213805086471, val_loss : 1.0369484305381775\n",
      "3/300--train_loss : 1.060296020087074, val_loss : 1.0315228164196015\n",
      "4/300--train_loss : 1.0533619733417736, val_loss : 1.0254252254962921\n",
      "5/300--train_loss : 1.0454545652165133, val_loss : 1.0195705592632294\n",
      "6/300--train_loss : 1.0410201935207142, val_loss : 1.0137321054935455\n",
      "7/300--train_loss : 1.0330483492682963, val_loss : 1.0077040433883666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _releaseLock at 0x7fb65806c670>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/hts/.conda/envs/hts_car/lib/python3.8/logging/__init__.py\", line 227, in _releaseLock\n",
      "    def _releaseLock():\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8/300--train_loss : 1.0294412514742683, val_loss : 1.0017593383789063\n",
      "9/300--train_loss : 1.022147304871503, val_loss : 0.9958597898483277\n",
      "10/300--train_loss : 1.016039886895348, val_loss : 0.9891881287097931\n",
      "11/300--train_loss : 1.0105285013423246, val_loss : 0.9834442138671875\n",
      "12/300--train_loss : 1.0043346987051123, val_loss : 0.9765957534313202\n",
      "13/300--train_loss : 0.9945420763071846, val_loss : 0.9697878539562226\n",
      "14/300--train_loss : 0.9886540244607365, val_loss : 0.9628623187541961\n",
      "15/300--train_loss : 0.9812769363908207, val_loss : 0.955944961309433\n",
      "16/300--train_loss : 0.972610196646522, val_loss : 0.948851215839386\n",
      "17/300--train_loss : 0.9654985561090357, val_loss : 0.9412339150905609\n",
      "18/300--train_loss : 0.9573509307468638, val_loss : 0.9335070013999939\n",
      "19/300--train_loss : 0.9489740799455082, val_loss : 0.9259948909282685\n",
      "20/300--train_loss : 0.9417938905603745, val_loss : 0.9183698654174804\n",
      "21/300--train_loss : 0.9329359636587256, val_loss : 0.9105330348014832\n",
      "22/300--train_loss : 0.924370173145743, val_loss : 0.9020546317100525\n",
      "23/300--train_loss : 0.9155320104430703, val_loss : 0.8934597373008728\n",
      "24/300--train_loss : 0.9059036829892326, val_loss : 0.8850494980812073\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:1' if torch.cuda.is_available() else 'cpu'\n",
    "dataloaders = build_dataloader(train_batch_size=7, val_batch_size=3)\n",
    "model = NeuralNetwork().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.0001)\n",
    "num_epoch = 300\n",
    "\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "best_loss = 100\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    loss = train_one_epoch(dataloaders=dataloaders, model=model, optimizer=optimizer, device=device)\n",
    "    train_loss.append(loss['train'])\n",
    "    val_loss.append(loss['val'])\n",
    "    print(f\"{epoch+1}/{num_epoch}--train_loss : {loss['train']}, val_loss : {loss['val']}\")\n",
    "\n",
    "    if (loss['val']<best_loss):\n",
    "        best_loss = loss['val']\n",
    "        best_model = copy.deepcopy(model.state_dict())\n",
    "        os.makedirs('./trained_model/', exist_ok=True)\n",
    "        torch.save(best_model, os.path.join('./trained_model/', \"bestmodel.pt\"), _use_new_zipfile_serialization=False)\n",
    "    \n",
    "print(f\"bestmodel : {best_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "test_model = NeuralNetwork()\n",
    "test_model.load_state_dict(torch.load('./trained_model/bestmodel.pt'))\n",
    "test_model.eval()\n",
    "\n",
    "total_score = 0\n",
    "test_count = 0\n",
    "\n",
    "for index, batch in enumerate(dataloaders['val']):\n",
    "    x_data = batch[0]\n",
    "    y_data = batch[1]\n",
    "\n",
    "    pred = test_model(x_data)\n",
    "    pred = torch.argmax(pred, dim=1)\n",
    "    \n",
    "    for i,j in zip(pred,y_data):\n",
    "        test_count +=1\n",
    "        if i==j:\n",
    "            total_score += 1\n",
    "\n",
    "    \n",
    "print(total_score/test_count)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hts_car",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
