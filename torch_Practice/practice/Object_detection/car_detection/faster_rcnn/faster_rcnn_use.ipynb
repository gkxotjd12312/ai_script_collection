{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Class_Name_To_Int = {'Bus':0, 'Truck':1}\n",
    "IMAGE_SIZE = 448\n",
    "NUM_CLASSES = 2\n",
    "VERBOSE_FREQ = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/hts/A_project/hts_pytorch/data/DRIVING-DATASET/Detection/'\n",
    "train_data_dir = os.path.join(data_dir, 'train/')\n",
    "val_data_dir = os.path.join(data_dir, 'val/')\n",
    "\n",
    "csv_data = pd.read_csv(data_dir + 'df.csv')\n",
    "csv_data = csv_data.drop(columns=['Source', 'Confidence','IsOccluded','IsTruncated','IsGroupOf','IsDepiction','IsInside','XClick1X','XClick2X','XClick3X','XClick4X','XClick1Y','XClick2Y','XClick3Y','XClick4Y'])\n",
    "csv_data = csv_data[['ImageID', 'LabelName', 'XMin', 'YMin', 'XMax', 'YMax']]\n",
    "train_data_list = os.listdir(train_data_dir)\n",
    "val_data_list = os.listdir(val_data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                ImageID LabelName      XMin      YMin      XMax      YMax\n",
      "0      0000599864fd15b3       Bus  0.343750  0.156162  0.908750  0.650047\n",
      "1      00006bdb1eb5cd74     Truck  0.276667  0.141604  0.697500  0.437343\n",
      "2      00006bdb1eb5cd74     Truck  0.702500  0.204261  0.999167  0.409774\n",
      "3      00010bf498b64bab       Bus  0.156250  0.269188  0.371250  0.705228\n",
      "4      00013f14dd4e168f       Bus  0.287500  0.194184  0.999375  0.999062\n",
      "...                 ...       ...       ...       ...       ...       ...\n",
      "24057  fff2b15ad6007d0e     Truck  0.277344  0.226389  0.622656  0.859722\n",
      "24058  fff376d20410e4c9       Bus  0.295625  0.306667  0.558750  0.397500\n",
      "24059  fff376d20410e4c9       Bus  0.348125  0.423333  0.701250  0.744167\n",
      "24060  fffde5953a818927       Bus  0.277500  0.565000  0.605625  0.795833\n",
      "24061  fffde5953a818927       Bus  0.613125  0.623333  0.828750  0.795833\n",
      "\n",
      "[24062 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(csv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data의 개수 : 13703\n",
      "  val data의 개수 : 1522\n"
     ]
    }
   ],
   "source": [
    "print(f'train data의 개수 : {len(train_data_list)}')\n",
    "print(f'  val data의 개수 : {len(val_data_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class car_data_set():\n",
    "    def __init__(self, data_dir, phase, csv_data, transformer = None):\n",
    "        self.csv_data = csv_data\n",
    "        self.phase_data_dir = (data_dir + phase + '/')\n",
    "        self.data_list = os.listdir(self.phase_data_dir)\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def get_label_def(self, image_name, img_H, img_W):\n",
    "        label = self.csv_data.loc[(self.csv_data['ImageID'] == image_name.split(\".\")[0])]\n",
    "        target_name = [Class_Name_To_Int[i] for i in label['LabelName'].values]\n",
    "        bounding_box = label.drop(columns = ['ImageID', 'LabelName']).values\n",
    "\n",
    "        bounding_box[:, [0,2]] *= img_W\n",
    "        bounding_box[:, [1,3]] *= img_H\n",
    "\n",
    "        return target_name, bounding_box\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_name = self.data_list[index]\n",
    "        \n",
    "        image = cv2.imread(self.phase_data_dir + image_name)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        img_H, img_W,_ = image.shape\n",
    "\n",
    "        if self.transformer:\n",
    "            image = self.transformer(image)\n",
    "            _, img_H, img_W = image.shape\n",
    "\n",
    "\n",
    "        target_name, bounding_box = self.get_label_def(image_name,img_H, img_W)\n",
    "\n",
    "        target = {}\n",
    "        target['boxes'] = torch.Tensor(bounding_box).float()\n",
    "        target['labels'] = torch.Tensor(target_name).long()\n",
    "        \n",
    "        \n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(size=(IMAGE_SIZE, IMAGE_SIZE) ,antialias=True),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    image_list = []\n",
    "    target_list = []\n",
    "    for a,b in batch:\n",
    "        image_list.append(a)\n",
    "        target_list.append(b)\n",
    "    \n",
    "    return image_list, target_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataloader(data_dir, train_batch_size = 4, val_batch_size = 4, image_size = 448):\n",
    "    dataloaders = {}\n",
    "\n",
    "    train_dataset = car_data_set(csv_data=csv_data,data_dir=data_dir, phase='train', transformer=transformer)\n",
    "    val_dataset = car_data_set(csv_data=csv_data,data_dir=data_dir, phase='val', transformer=transformer)\n",
    "    \n",
    "    dataloaders['train'] = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    dataloaders['val'] = DataLoader(val_dataset, batch_size=val_batch_size, shuffle=False, collate_fn=collate_fn)    \n",
    "\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'boxes': tensor([[147.5600,  97.0668, 361.2000, 296.4268]]), 'labels': tensor([1])}, {'boxes': tensor([[205.5200, 282.3780, 267.9600, 359.5935],\n",
      "        [162.9600, 273.7983, 199.3600, 324.9026]]), 'labels': tensor([1, 1])}]\n",
      "[{'boxes': tensor([[ 18.8124, 114.7816, 415.6248, 368.9410]]), 'labels': tensor([1])}, {'boxes': tensor([[124.0400, 200.2775, 353.6400, 362.7667]]), 'labels': tensor([1])}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dloaders = build_dataloader(data_dir, train_batch_size=2,val_batch_size=2, image_size=448)\n",
    "\n",
    "for phase in [\"train\", \"val\"]:\n",
    "    for index, batch in enumerate(dloaders[phase]):\n",
    "        images = batch[0]\n",
    "        targets = batch[1]\n",
    "        print(targets)\n",
    "        if index == 0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_classes):\n",
    "    #모델을 가져오고\n",
    "    model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    #fasterrcnn모델은 roi해서 각 클래스 개수, 개수*4의 바운딩박스를 가지는\n",
    "    #그중에 box_predictor를 수정해야한다 근데 cls_score의 in부분만 바꾸면 bounding박스가바뀐다.\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hts/.conda/envs/hts_car/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/hts/.conda/envs/hts_car/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = build_model(num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(dataloaders, model, optimizer, device):\n",
    "    train_loss = defaultdict(float)\n",
    "    val_loss = defaultdict(float)\n",
    "    \n",
    "    model.train() #faster-rcnn은 eval하니깐 오류생김\n",
    "    \n",
    "    for phase in [\"train\", \"val\"]:\n",
    "\n",
    "        for index, batch in enumerate(dataloaders[phase]):\n",
    "            images = batch[0]\n",
    "            targets = batch[1]\n",
    "    \n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            with torch.set_grad_enabled(phase == \"train\"):\n",
    "                loss = model(images, targets)\n",
    "            total_loss = sum(each_loss for each_loss in loss.values())\n",
    "            \n",
    "            if phase == \"train\":\n",
    "                optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if (index > 0) and (index % VERBOSE_FREQ) == 0:\n",
    "                    text = f\"{index}/{len(dataloaders[phase])} - \"\n",
    "                    for k, v in loss.items():\n",
    "                        text += f\"{k}: {v.item():.4f}  \"\n",
    "                    print(text)\n",
    "\n",
    "                for k, v in loss.items():\n",
    "                    train_loss[k] += v.item()\n",
    "                train_loss[\"total_loss\"] += total_loss.item()\n",
    "                \n",
    "            else:\n",
    "                for k, v in loss.items():\n",
    "                    val_loss[k] += v.item()\n",
    "                val_loss[\"total_loss\"] += total_loss.item()\n",
    "                \n",
    "    for k in train_loss.keys():\n",
    "        train_loss[k] /= len(dataloaders[\"train\"])\n",
    "        val_loss[k] /= len(dataloaders[\"val\"])\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = data_dir\n",
    "is_cuda = True\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "IMAGE_SIZE = 448\n",
    "BATCH_SIZE = 30\n",
    "VERBOSE_FREQ = 200\n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available and is_cuda else 'cpu')\n",
    "\n",
    "dataloaders = build_dataloader(data_dir=data_dir, train_batch_size=BATCH_SIZE, val_batch_size=BATCH_SIZE, image_size=IMAGE_SIZE)\n",
    "model = build_model(num_classes=NUM_CLASSES)\n",
    "model = model.to(DEVICE)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/457 - loss_classifier: 0.0389  loss_box_reg: 0.0354  loss_objectness: 0.0242  loss_rpn_box_reg: 0.0056  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/hts/A_project/hts_pytorch/git_Folder/ai_script_collection/torch_Practice/practice/Object_detection/car_detection/faster_rcnn_use.ipynb 셀 16\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B203.241.249.50/home/hts/A_project/hts_pytorch/git_Folder/ai_script_collection/torch_Practice/practice/Object_detection/car_detection/faster_rcnn_use.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m os\u001b[39m.\u001b[39mmakedirs(\u001b[39m'\u001b[39m\u001b[39m./trained_model/\u001b[39m\u001b[39m'\u001b[39m, exist_ok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B203.241.249.50/home/hts/A_project/hts_pytorch/git_Folder/ai_script_collection/torch_Practice/practice/Object_detection/car_detection/faster_rcnn_use.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B203.241.249.50/home/hts/A_project/hts_pytorch/git_Folder/ai_script_collection/torch_Practice/practice/Object_detection/car_detection/faster_rcnn_use.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     train_loss, val_loss \u001b[39m=\u001b[39m train_one_epoch(dataloaders\u001b[39m=\u001b[39;49mdataloaders, model\u001b[39m=\u001b[39;49mmodel, optimizer\u001b[39m=\u001b[39;49moptimizer, device\u001b[39m=\u001b[39;49mDEVICE)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B203.241.249.50/home/hts/A_project/hts_pytorch/git_Folder/ai_script_collection/torch_Practice/practice/Object_detection/car_detection/faster_rcnn_use.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     train_losses\u001b[39m.\u001b[39mappend(train_loss)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B203.241.249.50/home/hts/A_project/hts_pytorch/git_Folder/ai_script_collection/torch_Practice/practice/Object_detection/car_detection/faster_rcnn_use.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m     val_losses\u001b[39m.\u001b[39mappend(val_loss)\n",
      "\u001b[1;32m/home/hts/A_project/hts_pytorch/git_Folder/ai_script_collection/torch_Practice/practice/Object_detection/car_detection/faster_rcnn_use.ipynb 셀 16\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B203.241.249.50/home/hts/A_project/hts_pytorch/git_Folder/ai_script_collection/torch_Practice/practice/Object_detection/car_detection/faster_rcnn_use.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m targets \u001b[39m=\u001b[39m [{k: v\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m t\u001b[39m.\u001b[39mitems()} \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m targets]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B203.241.249.50/home/hts/A_project/hts_pytorch/git_Folder/ai_script_collection/torch_Practice/practice/Object_detection/car_detection/faster_rcnn_use.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mset_grad_enabled(phase \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B203.241.249.50/home/hts/A_project/hts_pytorch/git_Folder/ai_script_collection/torch_Practice/practice/Object_detection/car_detection/faster_rcnn_use.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m     loss \u001b[39m=\u001b[39m model(images, targets)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B203.241.249.50/home/hts/A_project/hts_pytorch/git_Folder/ai_script_collection/torch_Practice/practice/Object_detection/car_detection/faster_rcnn_use.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m total_loss \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(each_loss \u001b[39mfor\u001b[39;00m each_loss \u001b[39min\u001b[39;00m loss\u001b[39m.\u001b[39mvalues())\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B203.241.249.50/home/hts/A_project/hts_pytorch/git_Folder/ai_script_collection/torch_Practice/practice/Object_detection/car_detection/faster_rcnn_use.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mif\u001b[39;00m phase \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/.conda/envs/hts_car/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/hts_car/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/hts_car/lib/python3.8/site-packages/torchvision/models/detection/generalized_rcnn.py:104\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(features, torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    103\u001b[0m     features \u001b[39m=\u001b[39m OrderedDict([(\u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m, features)])\n\u001b[0;32m--> 104\u001b[0m proposals, proposal_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrpn(images, features, targets)\n\u001b[1;32m    105\u001b[0m detections, detector_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mroi_heads(features, proposals, images\u001b[39m.\u001b[39mimage_sizes, targets)\n\u001b[1;32m    106\u001b[0m detections \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform\u001b[39m.\u001b[39mpostprocess(detections, images\u001b[39m.\u001b[39mimage_sizes, original_image_sizes)  \u001b[39m# type: ignore[operator]\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/hts_car/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.conda/envs/hts_car/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/hts_car/lib/python3.8/site-packages/torchvision/models/detection/rpn.py:370\u001b[0m, in \u001b[0;36mRegionProposalNetwork.forward\u001b[0;34m(self, images, features, targets)\u001b[0m\n\u001b[1;32m    366\u001b[0m objectness, pred_bbox_deltas \u001b[39m=\u001b[39m concat_box_prediction_layers(objectness, pred_bbox_deltas)\n\u001b[1;32m    367\u001b[0m \u001b[39m# apply pred_bbox_deltas to anchors to obtain the decoded proposals\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[39m# note that we detach the deltas because Faster R-CNN do not backprop through\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[39m# the proposals\u001b[39;00m\n\u001b[0;32m--> 370\u001b[0m proposals \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbox_coder\u001b[39m.\u001b[39;49mdecode(pred_bbox_deltas\u001b[39m.\u001b[39;49mdetach(), anchors)\n\u001b[1;32m    371\u001b[0m proposals \u001b[39m=\u001b[39m proposals\u001b[39m.\u001b[39mview(num_images, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m4\u001b[39m)\n\u001b[1;32m    372\u001b[0m boxes, scores \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilter_proposals(proposals, objectness, images\u001b[39m.\u001b[39mimage_sizes, num_anchors_per_level)\n",
      "File \u001b[0;32m~/.conda/envs/hts_car/lib/python3.8/site-packages/torchvision/models/detection/_utils.py:178\u001b[0m, in \u001b[0;36mBoxCoder.decode\u001b[0;34m(self, rel_codes, boxes)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[39mif\u001b[39;00m box_sum \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    177\u001b[0m     rel_codes \u001b[39m=\u001b[39m rel_codes\u001b[39m.\u001b[39mreshape(box_sum, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m--> 178\u001b[0m pred_boxes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecode_single(rel_codes, concat_boxes)\n\u001b[1;32m    179\u001b[0m \u001b[39mif\u001b[39;00m box_sum \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    180\u001b[0m     pred_boxes \u001b[39m=\u001b[39m pred_boxes\u001b[39m.\u001b[39mreshape(box_sum, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m4\u001b[39m)\n",
      "File \u001b[0;32m~/.conda/envs/hts_car/lib/python3.8/site-packages/torchvision/models/detection/_utils.py:216\u001b[0m, in \u001b[0;36mBoxCoder.decode_single\u001b[0;34m(self, rel_codes, boxes)\u001b[0m\n\u001b[1;32m    213\u001b[0m pred_h \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mexp(dh) \u001b[39m*\u001b[39m heights[:, \u001b[39mNone\u001b[39;00m]\n\u001b[1;32m    215\u001b[0m \u001b[39m# Distance from center to box's corner.\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m c_to_c_h \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(\u001b[39m0.5\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mpred_ctr_y\u001b[39m.\u001b[39;49mdtype, device\u001b[39m=\u001b[39;49mpred_h\u001b[39m.\u001b[39;49mdevice) \u001b[39m*\u001b[39m pred_h\n\u001b[1;32m    217\u001b[0m c_to_c_w \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(\u001b[39m0.5\u001b[39m, dtype\u001b[39m=\u001b[39mpred_ctr_x\u001b[39m.\u001b[39mdtype, device\u001b[39m=\u001b[39mpred_w\u001b[39m.\u001b[39mdevice) \u001b[39m*\u001b[39m pred_w\n\u001b[1;32m    219\u001b[0m pred_boxes1 \u001b[39m=\u001b[39m pred_ctr_x \u001b[39m-\u001b[39m c_to_c_w\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 30\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "os.makedirs('./trained_model/', exist_ok=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, val_loss = train_one_epoch(dataloaders=dataloaders, model=model, optimizer=optimizer, device=DEVICE)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(f\"epoch : {epoch+1}/{num_epochs} - Train Loss : {train_loss['total_loss']:.4f}, Val Loss : {val_loss['total_loss']:.4f}\")\n",
    "\n",
    "    if(epoch +1) % 10 == 0:\n",
    "        torch.save(model, './trained_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hts_car",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
