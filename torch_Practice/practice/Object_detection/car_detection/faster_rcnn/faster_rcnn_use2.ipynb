{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from collections import defaultdict\n",
    "from ipywidgets import interact\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "from torchvision.ops import nms\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 시작 파라미터 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/hts/A_project/hts_pytorch/data/DRIVING-DATASET/Detection/'\n",
    "train_data_dir = os.path.join(data_dir, 'train/')\n",
    "val_data_dir = os.path.join(data_dir, 'val/')\n",
    "\n",
    "csv_data = pd.read_csv(data_dir + 'df.csv')\n",
    "csv_data = csv_data.drop(columns=['Source', 'Confidence','IsOccluded','IsTruncated','IsGroupOf','IsDepiction','IsInside','XClick1X','XClick2X','XClick3X','XClick4X','XClick1Y','XClick2Y','XClick3Y','XClick4Y'])\n",
    "csv_data = csv_data[['ImageID', 'LabelName', 'XMin', 'YMin', 'XMax', 'YMax']]\n",
    "train_data_list = os.listdir(train_data_dir)\n",
    "val_data_list = os.listdir(val_data_dir)\n",
    "\n",
    "Class_Name_To_Int = {'Bus':0, 'Truck':1}\n",
    "Int_To_Class_Name = {0:'Bus', 1:'Truck'}\n",
    "NUM_CLASSES = 2\n",
    "VERBOSE_FREQ = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 커스텀데이터셋 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class car_data_set():\n",
    "    def __init__(self, data_dir, phase, csv_data, transformer = None):\n",
    "        self.csv_data = csv_data\n",
    "        self.phase_data_dir = (data_dir + phase + '/')\n",
    "        self.data_list = os.listdir(self.phase_data_dir)\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def get_label_def(self, image_name, img_H, img_W):\n",
    "        label = self.csv_data.loc[(self.csv_data['ImageID'] == image_name.split(\".\")[0])]\n",
    "        target_name = [Class_Name_To_Int[i] for i in label['LabelName'].values]\n",
    "        bounding_box = label.drop(columns = ['ImageID', 'LabelName']).values\n",
    "        bounding_box[:, [0,2]] *= img_W\n",
    "        bounding_box[:, [1,3]] *= img_H\n",
    "\n",
    "        return target_name, bounding_box\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_name = self.data_list[index]\n",
    "        \n",
    "        \n",
    "        image = cv2.imread(self.phase_data_dir + image_name)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        img_H, img_W,_ = image.shape\n",
    "\n",
    "        if self.transformer:\n",
    "            image = self.transformer(image)\n",
    "            _, img_H, img_W = image.shape\n",
    "\n",
    "\n",
    "        target_name, bounding_box = self.get_label_def(image_name,img_H, img_W)\n",
    "\n",
    "        target = {}\n",
    "        target['boxes'] = torch.Tensor(bounding_box).float()\n",
    "        target['labels'] = torch.Tensor(target_name).long()\n",
    "        \n",
    "        \n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    image_list = []\n",
    "    target_list = []\n",
    "\n",
    "    for a,b in batch:\n",
    "        image_list.append(a)\n",
    "        target_list.append(b)\n",
    "    \n",
    "    return image_list, target_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataloader(data_dir,csv_data):\n",
    "    \n",
    "    dataloaders = {}\n",
    "    train_data_set = car_data_set(data_dir=data_dir, phase='train', csv_data=csv_data, transformer=transformer)\n",
    "    val_data_set = car_data_set(data_dir=data_dir, phase='val', csv_data=csv_data, transformer=transformer)\n",
    "\n",
    "    dataloaders['train'] = DataLoader(train_data_set, shuffle=True, collate_fn=collate_fn)\n",
    "    dataloaders['val'] = DataLoader(val_data_set, shuffle=False, collate_fn=collate_fn)\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이미지 데이터 확인 (transformer사용 X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_set = car_data_set(data_dir=data_dir, phase='train', csv_data=csv_data, transformer=None)\n",
    "\n",
    "#@interact(index=(0,len(train_data_set)-1))\n",
    "def show_sample(index=0):\n",
    "    images, target = temp_set[index]\n",
    "    boundingbox = target['boxes']\n",
    "    targetname = target['labels']\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(images)\n",
    "\n",
    "    for bbox,target in zip(boundingbox, targetname):\n",
    "        target = Int_To_Class_Name[target.item()]\n",
    "        xmin = bbox[0] \n",
    "        ymin = bbox[1]\n",
    "        ax.text(xmin, ymin, target,color='red' )\n",
    "        ax.add_patch(patches.Rectangle(\n",
    "            (xmin, ymin), bbox[2]-bbox[0], bbox[3]-bbox[1],\n",
    "            fill=False,\n",
    "            edgecolor = 'red',\n",
    "            linewidth=1))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 이미지 데이터 확인 (transformer사용 O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Resize(size=(448, 448)),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "temp_set = car_data_set(data_dir=data_dir, phase='train', csv_data=csv_data, transformer=transformer)\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "def unnormalize(image, mean, std):\n",
    "    mean = torch.tensor(mean).view(-1, 1, 1)\n",
    "    std = torch.tensor(std).view(-1, 1, 1)\n",
    "    image = image * std + mean\n",
    "    return image\n",
    "\n",
    "#@interact(index=(0,len(train_data_set)-1))\n",
    "def show_sample(index=0):\n",
    "    images, target = temp_set[index]\n",
    "    boundingbox = target['boxes']\n",
    "    targetname = target['labels']\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    images = unnormalize(image=images, mean=mean, std=std)\n",
    "    images = images.permute(1,2,0).numpy()\n",
    "    ax.imshow(images)\n",
    "\n",
    "    for bbox,target in zip(boundingbox, targetname):\n",
    "        target = Int_To_Class_Name[target.item()]\n",
    "        xmin = bbox[0] \n",
    "        ymin = bbox[1]\n",
    "        ax.text(xmin, ymin, target,color='red' )\n",
    "        ax.add_patch(patches.Rectangle(\n",
    "            (xmin, ymin), bbox[2]-bbox[0], bbox[3]-bbox[1],\n",
    "            fill=False,\n",
    "            edgecolor = 'red',\n",
    "            linewidth=1))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = models.detection.fasterrcnn_resnet50_fpn(pretrain=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, 2)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# one_epoch설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(dataloaders, model, optimizer, device):\n",
    "    train_loss = defaultdict(float)\n",
    "    val_loss = defaultdict(float)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for phase in ['train', 'val']:\n",
    "        for index, batch in enumerate(dataloaders[phase]):\n",
    "            images = batch[0]\n",
    "            targets = batch[1] \n",
    "\n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.item()} for t in targets]\n",
    "\n",
    "            with torch.set_grad_enabled(phase == 'train'):\n",
    "                loss = model(images, targets)\n",
    "            total_loss = sum(each_loss for each_loss in loss.values())\n",
    "\n",
    "            if phase == 'train':\n",
    "                optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if(index>0) and (index % 100) == 0:\n",
    "                    text = f\"{index}/{len(dataloaders[phase])} - \"\n",
    "                    for k,v in loss.items():\n",
    "                        text += f\"{k}: {v.item():.4f}\"\n",
    "                    print(text)\n",
    "                \n",
    "                for k, v in loss.items():\n",
    "                    train_loss[k] += v.item()\n",
    "                train_loss['total_loss'] += total_loss.item()\n",
    "            \n",
    "            else:\n",
    "                for k, v in loss.items():\n",
    "                    val_loss[k] += v.item()\n",
    "                val_loss['total_loss'] +=total_loss.item()\n",
    "    for k in train_loss.keys():\n",
    "        train_loss[k] /= len(dataloaders['train'])\n",
    "        val_loss[k] /= len(dataloaders['val'])\n",
    "    return train_loss, val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습 파라미터와 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /home/hts/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
      "100%|██████████| 97.8M/97.8M [00:09<00:00, 10.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "dataloaders = build_dataloader(data_dir=data_dir, csv_data=csv_data)\n",
    "model = build_model().to(device=device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hts/.conda/envs/hts_car/lib/python3.8/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/hts/A_project/hts_pytorch/git_Folder/ai_script_collection/torch_Practice/practice/Object_detection/car_detection/faster_rcnn_use2.ipynb 셀 18\u001b[0m line \u001b[0;36m7\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B203.241.249.50/home/hts/A_project/hts_pytorch/git_Folder/ai_script_collection/torch_Practice/practice/Object_detection/car_detection/faster_rcnn_use2.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m val_losses \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B203.241.249.50/home/hts/A_project/hts_pytorch/git_Folder/ai_script_collection/torch_Practice/practice/Object_detection/car_detection/faster_rcnn_use2.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epoch):\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B203.241.249.50/home/hts/A_project/hts_pytorch/git_Folder/ai_script_collection/torch_Practice/practice/Object_detection/car_detection/faster_rcnn_use2.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     train_loss, val_loss \u001b[39m=\u001b[39m train_one_epoch(dataloaders\u001b[39m=\u001b[39;49mdataloaders, model\u001b[39m=\u001b[39;49mmodel, optimizer\u001b[39m=\u001b[39;49moptimizer, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B203.241.249.50/home/hts/A_project/hts_pytorch/git_Folder/ai_script_collection/torch_Practice/practice/Object_detection/car_detection/faster_rcnn_use2.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     train_losses\u001b[39m.\u001b[39mappend(train_loss)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B203.241.249.50/home/hts/A_project/hts_pytorch/git_Folder/ai_script_collection/torch_Practice/practice/Object_detection/car_detection/faster_rcnn_use2.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     val_losses\u001b[39m.\u001b[39mappend(val_loss)\n",
      "\u001b[1;32m/home/hts/A_project/hts_pytorch/git_Folder/ai_script_collection/torch_Practice/practice/Object_detection/car_detection/faster_rcnn_use2.ipynb 셀 18\u001b[0m line \u001b[0;36m8\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B203.241.249.50/home/hts/A_project/hts_pytorch/git_Folder/ai_script_collection/torch_Practice/practice/Object_detection/car_detection/faster_rcnn_use2.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B203.241.249.50/home/hts/A_project/hts_pytorch/git_Folder/ai_script_collection/torch_Practice/practice/Object_detection/car_detection/faster_rcnn_use2.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m phase \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B203.241.249.50/home/hts/A_project/hts_pytorch/git_Folder/ai_script_collection/torch_Practice/practice/Object_detection/car_detection/faster_rcnn_use2.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     \u001b[39mfor\u001b[39;00m index, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(dataloaders[phase]):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B203.241.249.50/home/hts/A_project/hts_pytorch/git_Folder/ai_script_collection/torch_Practice/practice/Object_detection/car_detection/faster_rcnn_use2.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m         images \u001b[39m=\u001b[39m batch[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B203.241.249.50/home/hts/A_project/hts_pytorch/git_Folder/ai_script_collection/torch_Practice/practice/Object_detection/car_detection/faster_rcnn_use2.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m         targets \u001b[39m=\u001b[39m batch[\u001b[39m1\u001b[39m] \n",
      "File \u001b[0;32m~/.conda/envs/hts_car/lib/python3.8/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/hts_car/lib/python3.8/site-packages/torch/utils/data/dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.conda/envs/hts_car/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "\u001b[1;32m/home/hts/A_project/hts_pytorch/git_Folder/ai_script_collection/torch_Practice/practice/Object_detection/car_detection/faster_rcnn_use2.ipynb 셀 18\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B203.241.249.50/home/hts/A_project/hts_pytorch/git_Folder/ai_script_collection/torch_Practice/practice/Object_detection/car_detection/faster_rcnn_use2.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m image_list \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B203.241.249.50/home/hts/A_project/hts_pytorch/git_Folder/ai_script_collection/torch_Practice/practice/Object_detection/car_detection/faster_rcnn_use2.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m target_list \u001b[39m=\u001b[39m []\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B203.241.249.50/home/hts/A_project/hts_pytorch/git_Folder/ai_script_collection/torch_Practice/practice/Object_detection/car_detection/faster_rcnn_use2.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m a,b,c \u001b[39min\u001b[39;00m batch:\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B203.241.249.50/home/hts/A_project/hts_pytorch/git_Folder/ai_script_collection/torch_Practice/practice/Object_detection/car_detection/faster_rcnn_use2.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m     image_list\u001b[39m.\u001b[39mappend(a)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B203.241.249.50/home/hts/A_project/hts_pytorch/git_Folder/ai_script_collection/torch_Practice/practice/Object_detection/car_detection/faster_rcnn_use2.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     target_list\u001b[39m.\u001b[39mappend(b)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "num_epoch=30\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    train_loss, val_loss = train_one_epoch(dataloaders=dataloaders, model=model, optimizer=optimizer, device=device)\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    print(f\"epoch:{epoch+1}/{num_epoch} - Train Loss: {train_loss['total_loss']:.4f}, Val Loss: {val_loss['total_loss']:.4f}\")\n",
    "    \n",
    "    if (epoch+1) % 10 == 0:\n",
    "        os.makedirs('./trained_model/', exist_ok=True)\n",
    "        torch.save(model, os.path.join('./trained_model/','bestmodel.pt'),_use_new_zipfile_serialization=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 학습된 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = build_model()\n",
    "test_model.load_state_dict(torch.load('./trained_model/bestmodel.pt'))\n",
    "test_model.eval().to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 겹치는거 확인해주는 구간"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(prediction, conf_thres=0.2, iou_threshold = 0.1):\n",
    "    pred_box = prediction['boxes'].cpu().detach().numpy()\n",
    "    pred_label = prediction['labels'].cpu().detach().numpy()\n",
    "    pred_conf = prediction['scores'].cpu().detach().numpy()\n",
    "\n",
    "    valid_index = pred_conf>conf_thres\n",
    "    pred_box = pred_box[valid_index]\n",
    "    pred_label = pred_label[valid_index]\n",
    "    pred_conf = pred_conf[valid_index]\n",
    "\n",
    "    valid_index = nms(torch.tensor(pred_box.astype(np.float32)), torch.tensor(pred_conf), iou_threshold=iou_threshold)\n",
    "    pred_box = pred_box[valid_index.numpy()]\n",
    "    pred_conf = pred_conf[valid_index.numpy()]\n",
    "    pred_label = pred_label[valid_index.numpy()]\n",
    "    return np.concatenate((pred_box, pred_conf[:, np.newaxis], pred_label[:, np.newaxis]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for index, batch in enumerate(dataloaders['val']):\n",
    "    images = batch[0]\n",
    "    targets = batch[1]\n",
    "\n",
    "    images = list(image.to(device) for image in images)\n",
    "    targets = [{k: v.to(device) for k,v in t.items()} for t in targets]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        prediction = model(images)\n",
    "\n",
    "    image = images[0]\n",
    "    prediction = postprocess(prediction[0])\n",
    "    prediction[:, 2].clip(min=0, max = image.shape[1])\n",
    "    prediction[:, 3].clip(min=0, max = image.shape[0])\n",
    "\n",
    "    plt.imshow(image)\n",
    "    print(prediction)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    if index == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_sample(index=0):\n",
    "    images, target = temp_set[index]\n",
    "    boundingbox = target['boxes']\n",
    "    targetname = target['labels']\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    images = unnormalize(image=images, mean=mean, std=std)\n",
    "    images = images.permute(1,2,0).numpy()\n",
    "    ax.imshow(images)\n",
    "\n",
    "    for bbox,target in zip(boundingbox, targetname):\n",
    "        target = Int_To_Class_Name[target.item()]\n",
    "        xmin = bbox[0] \n",
    "        ymin = bbox[1]\n",
    "        ax.text(xmin, ymin, target,color='red' )\n",
    "        ax.add_patch(patches.Rectangle(\n",
    "            (xmin, ymin), bbox[2]-bbox[0], bbox[3]-bbox[1],\n",
    "            fill=False,\n",
    "            edgecolor = 'red',\n",
    "            linewidth=1))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMS적용"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hts_car",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
