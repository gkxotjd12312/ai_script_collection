{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "Class_Name_To_Int = {'Bus':0, 'Truck':1}\n",
    "IMAGE_SIZE = 448\n",
    "NUM_CLASSES = 2\n",
    "VERBOSE_FREQ = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/home/hts/A_project/hts_pytorch/data/DRIVING-DATASET/Detection/'\n",
    "train_data_dir = os.path.join(data_dir, 'train/')\n",
    "val_data_dir = os.path.join(data_dir, 'val/')\n",
    "\n",
    "csv_data = pd.read_csv(data_dir + 'df.csv')\n",
    "csv_data = csv_data.drop(columns=['Source', 'Confidence','IsOccluded','IsTruncated','IsGroupOf','IsDepiction','IsInside','XClick1X','XClick2X','XClick3X','XClick4X','XClick1Y','XClick2Y','XClick3Y','XClick4Y'])\n",
    "csv_data = csv_data[['ImageID', 'LabelName', 'XMin', 'YMin', 'XMax', 'YMax']]\n",
    "train_data_list = os.listdir(train_data_dir)\n",
    "val_data_list = os.listdir(val_data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                ImageID LabelName      XMin      YMin      XMax      YMax\n",
      "0      0000599864fd15b3       Bus  0.343750  0.156162  0.908750  0.650047\n",
      "1      00006bdb1eb5cd74     Truck  0.276667  0.141604  0.697500  0.437343\n",
      "2      00006bdb1eb5cd74     Truck  0.702500  0.204261  0.999167  0.409774\n",
      "3      00010bf498b64bab       Bus  0.156250  0.269188  0.371250  0.705228\n",
      "4      00013f14dd4e168f       Bus  0.287500  0.194184  0.999375  0.999062\n",
      "...                 ...       ...       ...       ...       ...       ...\n",
      "24057  fff2b15ad6007d0e     Truck  0.277344  0.226389  0.622656  0.859722\n",
      "24058  fff376d20410e4c9       Bus  0.295625  0.306667  0.558750  0.397500\n",
      "24059  fff376d20410e4c9       Bus  0.348125  0.423333  0.701250  0.744167\n",
      "24060  fffde5953a818927       Bus  0.277500  0.565000  0.605625  0.795833\n",
      "24061  fffde5953a818927       Bus  0.613125  0.623333  0.828750  0.795833\n",
      "\n",
      "[24062 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(csv_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data의 개수 : 13703\n",
      "  val data의 개수 : 1522\n"
     ]
    }
   ],
   "source": [
    "print(f'train data의 개수 : {len(train_data_list)}')\n",
    "print(f'  val data의 개수 : {len(val_data_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class car_data_set():\n",
    "    def __init__(self, data_dir, phase, csv_data, transformer = None):\n",
    "        self.csv_data = csv_data\n",
    "        self.phase_data_dir = (data_dir + phase + '/')\n",
    "        self.data_list = os.listdir(self.phase_data_dir)\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "    \n",
    "    def get_label_def(self, image_name, img_H, img_W):\n",
    "        label = self.csv_data.loc[(self.csv_data['ImageID'] == image_name.split(\".\")[0])]\n",
    "        target_name = [Class_Name_To_Int[i] for i in label['LabelName'].values]\n",
    "        bounding_box = label.drop(columns = ['ImageID', 'LabelName']).values\n",
    "\n",
    "        bounding_box[:, [0,2]] *= img_W\n",
    "        bounding_box[:, [1,3]] *= img_H\n",
    "\n",
    "        return target_name, bounding_box\n",
    "    \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_name = self.data_list[index]\n",
    "        \n",
    "        image = cv2.imread(self.phase_data_dir + image_name)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        img_H, img_W,_ = image.shape\n",
    "\n",
    "        if self.transformer:\n",
    "            image = self.transformer(image)\n",
    "            _, img_H, img_W = image.shape\n",
    "\n",
    "\n",
    "        target_name, bounding_box = self.get_label_def(image_name,img_H, img_W)\n",
    "\n",
    "        target = {}\n",
    "        target['boxes'] = torch.Tensor(bounding_box).float()\n",
    "        target['labels'] = torch.Tensor(target_name).long()\n",
    "        \n",
    "        \n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize(size=(IMAGE_SIZE, IMAGE_SIZE) ,antialias=True),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    image_list = []\n",
    "    target_list = []\n",
    "    for a,b in batch:\n",
    "        image_list.append(a)\n",
    "        target_list.append(b)\n",
    "    \n",
    "    return image_list, target_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataloader(data_dir, train_batch_size = 4, val_batch_size = 4, image_size = 448):\n",
    "    dataloaders = {}\n",
    "\n",
    "    train_dataset = car_data_set(csv_data=csv_data,data_dir=data_dir, phase='train', transformer=transformer)\n",
    "    val_dataset = car_data_set(csv_data=csv_data,data_dir=data_dir, phase='val', transformer=transformer)\n",
    "    \n",
    "    dataloaders['train'] = DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    dataloaders['val'] = DataLoader(val_dataset, batch_size=val_batch_size, shuffle=False, collate_fn=collate_fn)    \n",
    "\n",
    "    return dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'boxes': tensor([[  7.0000,  46.6668, 387.0998, 414.4000]]), 'labels': tensor([1])}, {'boxes': tensor([[173.0400, 268.1276, 247.8000, 360.1651],\n",
      "        [261.5200, 300.4879, 328.7200, 397.1484]]), 'labels': tensor([1, 1])}]\n",
      "[{'boxes': tensor([[ 18.8124, 114.7816, 415.6248, 368.9410]]), 'labels': tensor([1])}, {'boxes': tensor([[124.0400, 200.2775, 353.6400, 362.7667]]), 'labels': tensor([1])}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dloaders = build_dataloader(data_dir, train_batch_size=2,val_batch_size=2, image_size=448)\n",
    "\n",
    "for phase in [\"train\", \"val\"]:\n",
    "    for index, batch in enumerate(dloaders[phase]):\n",
    "        images = batch[0]\n",
    "        targets = batch[1]\n",
    "        print(targets)\n",
    "        if index == 0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(num_classes):\n",
    "    #모델을 가져오고\n",
    "    model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    #fasterrcnn모델은 roi해서 각 클래스 개수, 개수*4의 바운딩박스를 가지는\n",
    "    #그중에 box_predictor를 수정해야한다 근데 cls_score의 in부분만 바꾸면 bounding박스가바뀐다.\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hts/.conda/envs/hts_car/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/hts/.conda/envs/hts_car/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /home/hts/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n",
      "100%|██████████| 160M/160M [00:14<00:00, 11.5MB/s] \n"
     ]
    }
   ],
   "source": [
    "model = build_model(num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(dataloaders, model, optimizer, device):\n",
    "    total_loss = {'train': 0, 'val':0}\n",
    "    loss = 0.0\n",
    "\n",
    "    for phase in ['train', 'val']:\n",
    "        running_loss = 0.0\n",
    "\n",
    "        if phase =='train':\n",
    "            model.train()\n",
    "        elif phase =='val':\n",
    "            model.eval()\n",
    "        \n",
    "        for index, batch in enumerate(dataloaders[phase]):\n",
    "            images = batch[0]\n",
    "            targets = batch[1]\n",
    "            filenames = batch[2]\n",
    "    \n",
    "            images = list(image.to(device) for image in images)\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            with torch.set_grad_enabled(phase == \"train\"):\n",
    "                loss = model(images, targets)\n",
    "            total_loss = sum(each_loss for each_loss in loss.values())\n",
    "            \n",
    "            if phase == \"train\":\n",
    "                optimizer.zero_grad()\n",
    "                total_loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if (index > 0) and (index % VERBOSE_FREQ) == 0:\n",
    "                    text = f\"{index}/{len(dataloaders[phase])} - \"\n",
    "                    for k, v in loss.items():\n",
    "                        text += f\"{k}: {v.item():.4f}  \"\n",
    "                    print(text)\n",
    "\n",
    "                for k, v in loss.items():\n",
    "                    train_loss[k] += v.item()\n",
    "                train_loss[\"total_loss\"] += total_loss.item()\n",
    "\n",
    "            else:\n",
    "                for k, v in loss.items():\n",
    "                    val_loss[k] += v.item()\n",
    "                val_loss[\"total_loss\"] += total_loss.item()\n",
    "                    \n",
    "            running_loss += loss.item()\n",
    "        total_loss[phase] = running_loss/len(dataloaders[phase])\n",
    "    return total_loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hts_car",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
